ID	url	date	citation	application	training	architecture	parent	link_info	name	params(M)	cifar10	cifar100	SVHN	imageNet val top1	imagenet val top5	model path
leNet	http://ieeexplore.ieee.org/abstract/document/726791/	1998.11.01	10831	CV=>recognition	3.2.1.SGD;	2.2.1.1.plain;2.2.2.2.2.sigmoid;										
alexNet	https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks	2012.12.03	18438	CV=>recognition	3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;2.2.2.2.1.1.standard relu	2.2.1.1.plain;2.2.2.2.1.1.standard relu;	leNet		alexNet	60	na	na	na	36.7	15.4	
VGG	https://arxiv.org/abs/1409.1556	2014.09.04	8205	CV=>recognition	3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;2.2.2.2.1.1.standard relu	2.2.1.1.plain;2.2.2.2.1.1.standard relu;	alexNet	deeper;conv7=>2*conv3	vgg19	144	na	na	na	25.5	7.3	x
									vgg16	138				25.6	8.1	
NIN	https://arxiv.org/abs/1312.4400	2013.12.16	1159	CV=>recognition	3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;3.5.2.auxiliar classifier;2.2.2.2.1.1.standard relu	2.2.1.1.plain;2.2.2.1.6.mlpconv;2.2.2.2.1.1.standard relu	alexNet	conv=>mlpconv;FCN=>global average pooling	network in network		8.81	35.68	2.35			
inception	https://arxiv.org/abs/1409.4842	2014.09.17	5591	CV=>recognition	3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;2.2.2.2.1.1.standard relu	2.2.1.3.multi-branch;2.2.2.2.1.1.standard relu;	alexNet		inception	6.8	na	na	na	na	6.67	
							VGG	less computation cost								
DSN	https://arxiv.org/abs/1409.5185	2014.09.18	558	CV=>recognition	3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;3.5.2.auxiliar classifier;	2.2.1.1.plain;2.2.2.2.1.1.standard relu;			deeply supervised nets		8.22	34.57	1.92			
inception_rethink	https://arxiv.org/abs/1512.00567	2015.12.02	794	CV=>recognition	3.4.6.LSR;3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;	2.2.1.3.multi-branch;2.2.2.2.1.1.standard relu;	inception	easy to scale up	inception_v3	23.8	na	na	na	18.77	4.2	x
									inception_v2							
PRule-nets	https://arxiv.org/abs/1502.01852	2015.02.06	1981	CV=>recognition	3.1.7.he_normal;2.2.2.3.4.SPP;3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;	2.2.1.1.plain;2.2.2.2.1.5.PRelu;	VGG	enables to train extremely deep rectified models directly from scratch	PRule-nets						4.94	
fitNets	https://arxiv.org/abs/1412.6550	2014.12.19	274	CV=>recognition	3.5.1.knowledge distilling;3.5.3.intermediate hints;	2.2.1.1.plain;2.2.2.2.1.1.standard relu;	DSN	 a smoother hint	fitNets	2.5	8.39	35.04				
highwayNets	https://arxiv.org/pdf/1505.00387.pdf	2015.11.03	320	CV=>recognition		2.2.1.2.residual;2.2.2.2.1.1.standard relu;	LSTM	strongly inspired by it	highwayNets	2.3	7.76	na	na	na	na	
							fitNets	easier to train								
							VGG	enables to train extremely deep models directly from scratch								
resNet_v1	https://arxiv.org/abs/1512.03385	2015.12.10	5464	CV=>recognition	3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;3.1.7.he_normal;	2.2.1.2.residual;2.2.2.2.1.1.standard relu;	VGG	add residual connections, deeper	resNet_v1_56_cifar	0.86	6.97	25.16	na	na	na	x
							highwayNets	no gate	resNet_v1_110_cifar	1.7	6.43	25.16	na	na	na	x
									resNet_v1_50	25.6	na	na	na	24.1	7.1	x
									resNet_v1_152	60.4	na	na	na	21.43	5.71	x
resNet_v2	https://arxiv.org/abs/1603.05027	2016.05.16	575	CV=>recognition	3.4.7.pre-activation; 3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;	2.2.1.2.residual;2.2.2.2.1.1.standard relu;	resNet_v1	pre-activation makes training easier and improves generalization	resNet_v2	na	na	na	na	35.29	na	
									resNet_v2_56_cifar	1.6	6.99	na	na	na	na	
									resNet_v2_110_cifar	3.3	6.38	24.64	na	na	na	
inception_resNet	https://arxiv.org/abs/1602.07261	2016.02.23	480	CV=>recognition	3.2.3.RMSprop;scaling of the residuals;3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;	2.2.1.2.residual;2.2.1.3.multi-branch;2.2.2.2.1.1.standard relu;	inception_rethink	combine resnet with inception	inception_resNet	55.8	na	na	na	17.8	3.7	x
							resNet_v1	combine resnet with inception								
WRN	https://arxiv.org/abs/1605.07146	2016.05.23	285	CV=>recognition	3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;3.4.1.dropout;	2.2.1.2.residual;2.2.2.2.1.1.standard relu;	resNet_v1	wider instead of deeper for computation efficiency	wideResNet_16_4	2.9	5.24	23.91	1.64	na	na	x
									wideResNet_28_10	36	3.89	18.85	1.64	na	na	x
									wideResNet-50-2	na	na	na	na	21.9	5.79	
RIR	https://arxiv.org/abs/1603.08029	2016.05.25	37	CV=>recognition	3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;3.4.1.dropout;	2.2.1.2.residual;2.2.2.2.1.1.standard relu;	resNet_v1	generalizes ResNets and standard CNNs	resNet-in-resent	na	5.01	22.9	na	na	na	
fractalNet	https://arxiv.org/abs/1605.07648	2016.05.24	71	CV=>recognition	3.4.3.drop path; 3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;3.4.1.dropout;	2.2.1.3.multi-branch;2.2.2.2.1.1.standard relu;	resNet_v1	shows that explicit residual learning is not a requirement for building ultra-deep neural networks	fractalNet-40	22.9	5.24	22.49	na	na	na	x
									fractalNet-20	38.6	5.22	23.3	na	na	na	
resNeXt	https://arxiv.org/abs/1611.05431	2016.11.16	134	CV=>recognition	3.4.7.pre-activation; 3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;	2.2.1.2.residual;2.2.1.3.multi-branch;2.2.2.2.1.1.standard relu;	resNet_v1	add a new dimension called "â€œcardinality"	ResNeXt	25	3.58	17.31		20.4	5.3	
							inception	exploiting the split-transform-merge strategy in an easy, extensible way								
denseNet	https://arxiv.org/abs/1608.06993	2016.08.25	378	CV=>recognition	3.4.1.dropout;3.2.1.SGD with nesterov momentum;3.4.2.weight decay;3.1.7.he_normal;	2.2.1.2.residual;2.2.2.2.1.1.standard relu;	resNet_v1	dense residual connection; change add to concate 	denseNet_40_12	1	5.24	24.42	1.79	na	na	x
									denseNet_100_12	7.2	4.1	20.2	1.67	na	na	x
									denseNet_100_24	27.2	3.74	19.25	1.59	na	na	x
									denseNet-BC(l=100, k=12)	0.8	4.51	22.27	1.76	na	na	
									denseNet-BC(l=250, k=24)	15.3	3.62	17.6	1.74	na	na	
									denseNet-BC(l=190, k=40)	25.6	3.46	17.18	na	na	na	
									denseNet_121	8.1	na	na	na	25.02	7.71	x
									denseNet_201	20.2	na	na	na	22.58	6.34	x
squeezeNet	https://arxiv.org/abs/1602.07360	2016.02.24	264	CV=>recognition	3.4.1.dropout;3.2.1.SGD with momentum;3.4.2.weight decay;	2.2.1.3.multi-branch;2.2.2.2.1.1.standard relu;2.2.1.2.residual;	alexNet	same performance with 50x less parameters	squeezeNet	1.24	na	na	na	39.6	17.5	x
							inception	exploiting the split-transform-merge strategy in an easy, extensible way								
diracNet	https://arxiv.org/abs/1706.00388	2017.06.07		CV=>recognition	3.1.11.dirac;3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;3.4.1.dropout;	2.2.1.1.plain;2.2.2.2.1.4.CRelu;	resNet_v1	train a deep network without residual connections	diracNet-28-5	9.1	3.16	23.44				
									diracNet-28-10	36.5	4.75	21.54				
									diracNet-18	11.7				30.37	10.88	
									diracNet-34	21.8				27.79	9.34	
nasNet	https://arxiv.org/abs/1707.07012	2017.12.01	17	CV=>recognition	neural architecture search by reinforcement learning	2.2.1.2.residual;2.2.1.3.multi-branch;			nasNet_cifar	3.3	2.65	na	na	na	na	x
									nasNet_small	88.9	na	na	na	17.3	3.8	x
									nasNet_large	5.3	na	na	na	26	8.4	x
Elu	https://arxiv.org/abs/1511.07289	2015.11.23	493	CV=>recognition	3.4.4.batch normalization;3.2.1.SGD with momentum;3.4.2.weight decay;2.2.2.3.4.SPP; 3.4.1.dropout;		PRule-nets	 a noise-robust deactivation state	exponential linear unit		6.55	24.28				
FMP																
																
ID	url	date	citation	application	training	architecture	parent	link_info	name	params(M)	cifar10	cifar100	SVHN	imageNet val top1	imagenet val top5	model path
fcn	https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf	2015.06.01	3141	CV=>segmentation					fcn		67.2					
mask r-cnn	https://arxiv.org/abs/1703.06870	2017.04.05	88	CV=>segmentation					mask r-cnn							
deepLab_v1	https://arxiv.org/abs/1412.7062	2014.12.22	1183	CV=>segmentation					deepLab_v1		71.6					
deepLab_v2	https://arxiv.org/abs/1606.00915	2016.06.02	1183	CV=>segmentation					deepLab_v2		79.7					
uNet	https://arxiv.org/abs/1505.04597	2015.05.18	994	CV=>segmentation					uNet							
segNet	https://arxiv.org/abs/1511.00561	2015.11.02	464	CV=>segmentation					segNet		59.9					
dilated conv	https://arxiv.org/abs/1511.07122	2015.11.23	508	CV=>segmentation					dilated conv		75.3					
refineNet	https://arxiv.org/abs/1611.06612	2016.11.20	60	CV=>segmentation					refineNet		84.2					
PSPNet	https://arxiv.org/abs/1612.01105	2016.12.04	129	CV=>segmentation					PSPNet		85.4					
GCN	https://arxiv.org/abs/1703.02719	2017.03.08	15	CV=>segmentation					GCN		83.6					
deepLab_v3	https://arxiv.org/abs/1706.05587	2017.06.17	17	CV=>segmentation					deepLab_v3		85.7					
																
																
ID	url	date	citation	application	training	architecture	parent	link_info	name	params(M)	voc2007 mAP (train on 07 + 12 )	voc2012 mAP	COCO mAP@0.5 (test-dev)	COCO mAP@[0.5:0.95]	voc2007 fps	model path
r-cnn	http://arxiv.org/abs/1311.2524	2013.11.11	4309	CV=>detection					r-cnn		66	53.3				
SPP_net	http://arxiv.org/abs/1406.4729	2014.06.18	889	CV=>detection					SPP_net		59.2					
fast r-cnn	http://arxiv.org/abs/1504.08083	2015.04.30	1823	CV=>detection					fast r-cnn		71.8	68.4			0.5	
faster r-cnn(rpn, 300)	http://arxiv.org/abs/1506.01497	2015.06.04	2395	CV=>detection					faster r-cnn(rpn, 300)		73.2	70.4	42.7	21.9	7	
SSD	https://arxiv.org/abs/1512.02325	2015.12.08	654	CV=>detection					SSD		77.2	74.9	46.5	26.8	59	
YOLO	http://arxiv.org/abs/1506.02640	2015.06.08	881	CV=>detection					YOLO		66.4	57.9			155	
									YOLO+fast-rcnn			70.7				
R-FCN	https://arxiv.org/abs/1605.06409	2016.05.20	264	CV=>detection					R-FCN		80.5	77.6	53.2	31.5		
																
ID	url	date	citation	application	training	architecture	parent	link_info	name	params(M)	BLEU WMT2014 English2German	BLEU WMT14 English2French	BELU WMT16 English2Romanian			model path
Seq2Seq	https://arxiv.org/abs/1409.3215	2014.09.10	2829	NLP=>translation		stacked LSTM;			Seq2Seq			34.81				
GNMT	https://arxiv.org/abs/1609.08144	2016.09.26	369	NLP=>translation		stacked LSTM;attention;residual			GNMT		24.17	38.39				
conv seq2seq	https://arxiv.org/abs/1705.03122	2017.05.08	73	NLP=>translation		conv;attention;			conv seq2seq		25.13	40.51	30.02			
RNNsearch	https://arxiv.org/abs/1409.0473	2014.09.01	2362	NLP=>translation		soft attention; stacked;bidirectional;GRU;			RNNsearch			33.3				
RNNencdec	https://arxiv.org/abs/1406.1078	2014.06.03	1942	NLP=>translation		GRU;			RNNencdec			26.71				
																
ID	url	date	citation	application	training	architecture	parent	link_info	name	params(M)	KTH1	UCF101(RGB based, video)	HMDB51	Sports-1M	MPII	model path
LTC-CNN	https://arxiv.org/abs/1604.04494v1	2016.04.15	69	CV=>action		plain; 3D conv;			LTC-CNN			81.5	67.2			
3D conv+LSTM	https://link.springer.com/chapter/10.1007/978-3-642-25446-8_4	2011.01.01	251	CV=>action		plain; 3D conv;LSTM			3D conv+LSTM		94.39					
deepVideo	http://ieeexplore.ieee.org/document/6909619/	2014.06.23	1669	CV=>action		conv; fusion strategy			deepVideo					63.9		
AttnP+R101	https://arxiv.org/abs/1711.01467	2017.11.14	3	CV=>action		attentional pooling;resnet101;			AttnP+R101				50.8		30.3	
AttnP+I_V2	https://arxiv.org/abs/1711.01468	2017.11.14	3	CV=>action		attentional pooling;inception_BN;			AttnP+I_V2						26.2	
two-stream	https://arxiv.org/abs/1406.2199	2014.06.29	1185	CV=>action		plain;two-stream			two-stream			87	55.4			
3D conv	ieeexplore.ieee.org/document/6165309/	2013.03.06	1252	CV=>action		3D conv			3D conv							
LRCN	https://arxiv.org/abs/1411.4389	2014.11.17	1278	CV=>action;CV=>caption		2d conv; LSTM			LRCN			82.66				
ST-ResNet	https://arxiv.org/abs/1611.02155	2016.11.07	67	CV=>action		resnet;two-stream			ST-ResNet			93.46	66.41			